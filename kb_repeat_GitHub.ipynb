{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to reproduce the findings in the paper:\n",
    "S.Sarica & J.Luo. Knowledge Burden and the Future of Innovation with Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Takes a very long time to run (a week maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import gensim\n",
    "import base64\n",
    "import itertools\n",
    "import random\n",
    "import shutil\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the word2vec model that is trained on patent database\n",
    "Refer to S. Sarica, J. Luo, K. L. Wood, TechNet: Technology semantic network based on patent data. Expert Syst. Appl. 142 (2020), doi:10.1016/j.eswa.2019.112995."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"../../Full_patent_w2v_model_th02_min2_s600_w10.txt\"\n",
    "# you can download this model (~ 28GB) from following dropbox folder:\n",
    "# https://www.dropbox.com/sh/yu2z4sf3bcmkqrb/AABun5qFT8XZncHipEB1TDj0a?dl=0\n",
    "# download word_embeddings_*.txt files only to a sub-folder, \n",
    "# run the following cell to create a single file to be imported\n",
    "# then delete the downloaded files\n",
    "\n",
    "download_folder = './600/'\n",
    "num_files = 404\n",
    "\n",
    "with open (\"./data/word_embeddings.txt\", \n",
    "           'w', encoding = 'utf-8') as f:\n",
    "    for i in range(num_files):\n",
    "        with open(download_folder+'word_embeddings_'+ str(i)+'.txt', 'r', encoding = 'utf-8') as f1:\n",
    "            temp = f1.readlines()\n",
    "        print(f'{i+1}/404 done!}')\n",
    "        f.writelines(temp)\n",
    "#delete the download folder\n",
    "shutil.rmtree('./600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert that single file to word2vec format and import it\n",
    "model_path = datapath(os.getcwd()+\"/data/word_embeddings.txt\")\n",
    "tmp_file = get_tmpfile(os.getcwd()+'/Full_patent_w2v_model_th02_min2_s600_w10')\n",
    "glove2word2vec(model_path, tmp_file)\n",
    "#delete the unnecessary file\n",
    "os.remove(\"./data/word_embeddings.txt\") \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(tmp_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vocabulary:index dictionary\n",
    "with open('./data/vocab_index_tn01.pkl', 'rb') as f:\n",
    "    vocab_index = pickle.load(f)\n",
    "index_vocab = {item[1]:item[0] for item in vocab_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dictionary includes a list for each vocabulary index where list\n",
    "#counts the number of occurrence of the corresponding term years\n",
    "#from 1976 to 2017\n",
    "with open('./data/vocab_years_dict_TNv0.1_encoded.pkl', 'rb') as f:\n",
    "    vocab_years_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming arrays for collecting count statistics for concepts / year later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_terms = np.zeros(len(range(1976,2018))) #number of terms appearing in years\n",
    "year_new_terms = np.zeros(len(range(1977,2018))) #number of new terms in years\n",
    "year_new_terms_ = [[] for x in range(1976,2018)] #index of new terms in years\n",
    "for key in vocab_years_dict.keys():\n",
    "    temp = np.array(vocab_years_dict[key])\n",
    "    key_years = np.array([1 if temp[i]>0 else 0 for i in range(len(temp))])\n",
    "    year_terms = np.add(year_terms, key_years)\n",
    "    if temp[0]==0:\n",
    "        temp1 = np.where(key_years[1:] == 1)\n",
    "        if temp1[0].size>0:\n",
    "            year_new_terms[temp1[0][0]] += 1\n",
    "            year_new_terms_[temp1[0][0]+1].append(key)\n",
    "    else:\n",
    "        year_new_terms_[0].append(key)\n",
    "year_cumsum = np.cumsum([year_terms[0]]+year_new_terms.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year_cumsum and year_new_terms arrays hold the necessary information to\n",
    "generate the Fig. 1A in the manuscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity measurements\n",
    "We use different sample sizes 5000, 2000, 1000, 500<br>\n",
    "For each year we create 100 different samples<br>\n",
    "We then calculate the mean and standard deviation of these samples <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simInter(matrix, M):\n",
    "    N = len(matrix)\n",
    "    m = np.array([x[N-M:] for x in matrix[:N-M]])\n",
    "    return np.mean(m[m>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate semantic similarity for different sample sizes\n",
    "\n",
    "Ns = [5000, 2000, 1000, 500] #sample sizes\n",
    "iter_ = 100 #number of samples per year\n",
    "\n",
    "prop_new_cum = [year_new_terms[i]/year_cumsum[i+1] for i in range(4, 2017-1976)]\n",
    "prior_terms = list(itertools.chain.from_iterable(year_new_terms_[:5]))\n",
    "\n",
    "mean_sim = [[[0 for x in range(iter_)] for y in range(1981,2017)] for z in Ns]\n",
    "mean_sim_inter = [[[0 for x in range(iter_)] for y in range(1981,2017)] for z in Ns]\n",
    "\n",
    "\n",
    "for i in tqdm(range(2017-1981)):\n",
    "    #calculate number of new terms for this year\n",
    "    Ms = [int(np.ceil(prop_new_cum[i]*x)) for x in Ns]\n",
    "      \n",
    "    for j in range(iter_):\n",
    "        #select an initial random terms by using the largest sample size\n",
    "        priors = random.sample(prior_terms, Ns[0]-Ms[0])\n",
    "        news = random.sample(year_new_terms_[5+i], Ms[0])\n",
    "        total = priors+news\n",
    "        print(i, j)\n",
    "        matrix = np.zeros((Ns[0], Ns[0]))\n",
    "        \n",
    "        for k in range(Ns[0]):\n",
    "            for t in range(k+1, Ns[0]):\n",
    "                weight = model.similarity(index_vocab[total[k]], index_vocab[total[t]])\n",
    "                matrix[k][t] = weight\n",
    "                matrix[t][k] = weight\n",
    "        \n",
    "        mean_sim[0][i][j] = np.mean(matrix[matrix>0])\n",
    "        mean_sim_inter[0][i][j] = simInter(matrix, Ms[0])\n",
    "        \n",
    "        ind_ic = individual_IC(matrix, r)\n",
    "        for w in range(len(r)):\n",
    "            mean_ic[0][w][i][j] = np.mean([x[w] for x in ind_ic])\n",
    "            mean_ic_news[0][w][i][j] = np.mean([x[w] for x in ind_ic[:Ns[0]-Ms[0]]])\n",
    "        \n",
    "        for w in range(1,len(Ns)):\n",
    "            rand_priors = random.sample(priors, Ns[w]-Ms[w])\n",
    "            rand_news = random.sample(news, Ms[w])\n",
    "            rand_priors_inds = [priors.index(x) for x in rand_priors]\n",
    "            rand_news_inds = [news.index(x) for x in rand_news]\n",
    "            total_ind = rand_priors_inds + rand_news_inds\n",
    "            _matrix = np.array([[matrix[x][y] for x in total_ind] for y in total_ind])\n",
    "            mean_sim[w][i][j] = np.mean(_matrix[_matrix>0])\n",
    "            mean_sim_inter[w][i][j] = simInter(_matrix, Ms[w])\n",
    "                \n",
    "    \n",
    "    prior_terms += year_new_terms_[5+i]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_sim and mean_sim_inter arrays hold the information to generate the Fig.1B\n",
    "in the manuscript and Fig S3 in the supplementary material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean additional Information Content measurement\n",
    "We use different sample sizes 5000, 2000, 1000, 500<br>\n",
    "For each year we create 100 different samples<br>\n",
    "We then calculate the mean and standard deviation of these samples <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V= len(model.wv.vocab)\n",
    "cum_terms = {}\n",
    "most_prox_years = [[] for x in range(len(Ns))]\n",
    "K = 1000000\n",
    "\n",
    "for i in tqdm(range(2018-1976)):\n",
    "    if i>=5 and i<41:\n",
    "        temp = random.sample(year_new_terms_[i], Ns[0])\n",
    "        for j in tqdm(range(len(temp))):\n",
    "            try:\n",
    "                temp1 = model.wv.most_similar(index_vocab[temp[j]], topn = K)\n",
    "                for x in temp1:\n",
    "                    if cum_terms.get(vocab_index[x[0]]):\n",
    "                        most_prox.append(x[1])\n",
    "                        break\n",
    "            except:\n",
    "                temp1 = model.wv.most_similar(index_vocab[temp[j]], topn = min(K*10, V))\n",
    "                for x in temp1:\n",
    "                    if cum_terms.get(vocab_index[x[0]]):\n",
    "                        break\n",
    "        most_prox_years[0].append(most_prox)\n",
    "        for k in range(1,len(Ns)):\n",
    "            most_prox_years[k].append(random.sample(most_prox, Ns[k]))\n",
    "        \n",
    "    if i%10 == 0 and i>0:\n",
    "        K /= 10\n",
    "        K= int(K)\n",
    "    cum_terms.update({x:1 for x in year_new_terms_[i]})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log2 of the material in the array most_prox_years is used to produce\n",
    "Fig 1C and Fig S4 in the manuscript and supplementary materials, respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
